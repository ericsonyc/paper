
%% bare_adv.tex
%% V1.4
%% 2012/12/27
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the advanced use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8 or later) with an IEEE Computer
%% Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex,
%%                    bare_jrnl_transmag.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% IEEEtran V1.7 and later provides for these CLASSINPUT macros to allow the
% user to reprogram some IEEEtran.cls defaults if needed. These settings
% override the internal defaults of IEEEtran.cls regardless of which class
% options are used. Do not use these unless you have good reason to do so as
% they can result in nonIEEE compliant documents. User beware. ;)
%
%\newcommand{\CLASSINPUTbaselinestretch}{1.0} % baselinestretch
%\newcommand{\CLASSINPUTinnersidemargin}{1in} % inner side margin
%\newcommand{\CLASSINPUToutersidemargin}{1in} % outer side margin
%\newcommand{\CLASSINPUTtoptextmargin}{1in}   % top text margin
%\newcommand{\CLASSINPUTbottomtextmargin}{1in}% bottom text margin



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[12pt,journal,compsoc]{IEEEtran}
% The Computer Society requires 12pt.
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}


% For Computer Society journals, IEEEtran defaults to the use of
% Palatino/Palladio as is done in IEEE Computer Society journals.
% To go back to Times Roman, you can use this code:
%\renewcommand{\rmdefault}{ptm}\selectfont





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)



% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  % \usepackage[nocompress]{cite}
\else
  % normal IEEE
  % \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later.
%
% Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/tex-archive/info/epslatex/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%\usepackage{acronym}
% acronym.sty was written by Tobias Oetiker. This package provides tools for
% managing documents with large numbers of acronyms. (You don't *have* to
% use this package - unless you have a lot of acronyms, you may feel that
% such package management of them is bit of an overkill.)
% Do note that the acronym environment (which lists acronyms) will have a
% problem when used under IEEEtran.cls because acronym.sty relies on the
% description list environment - which IEEEtran.cls has customized for
% producing IEEE style lists. A workaround is to declared the longest
% label width via the IEEEtran.cls \IEEEiedlistdecl global control:
%
% \renewcommand{\IEEEiedlistdecl}{\IEEEsetlabelwidth{SONET}}
% \begin{acronym}
%
% \end{acronym}
% \renewcommand{\IEEEiedlistdecl}{\relax}% remember to reset \IEEEiedlistdecl
%
% instead of using the acronym environment's optional argument.
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/acronym/


%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/


%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and
% Axel Sommerfeldt. This package may be useful when used in conjunction with
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a
% page by themselves.


\usepackage{graphicx}
\usepackage{CJK}
\usepackage{float}
\usepackage{enumerate}
%\usepackage{enumitem}
\usepackage[justification=centering]{caption}
\usepackage{tabu}
\usepackage{array}
%\usepackage{overcite}
%\usepackage[super,square]{natbib}
\usepackage{type1cm}
%\usepackage[numbers,sort&compress]{natbib}
\usepackage{algorithm}
\usepackage{algpseudocode}


% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.


% NOTE: PDF thumbnail features are not required in IEEE papers
%       and their use requires extra complexity and work.
%\ifCLASSINFOpdf
%  \usepackage[pdftex]{thumbpdf}
%\else
%  \usepackage[dvips]{thumbpdf}
%\fi
% thumbpdf.sty and its companion Perl utility were written by Heiko Oberdiek.
% It allows the user a way to produce PDF documents that contain fancy
% thumbnail images of each of the pages (which tools like acrobat reader can
% utilize). This is possible even when using dvi->ps->pdf workflow if the
% correct thumbpdf driver options are used. thumbpdf.sty incorporates the
% file containing the PDF thumbnail information (filename.tpm is used with
% dvips, filename.tpt is used with pdftex, where filename is the base name of
% your tex document) into the final ps or pdf output document. An external
% utility, the thumbpdf *Perl script* is needed to make these .tpm or .tpt
% thumbnail files from a .ps or .pdf version of the document (which obviously
% does not yet contain pdf thumbnails). Thus, one does a:
%
% thumbpdf filename.pdf
%
% to make a filename.tpt, and:
%
% thumbpdf --mode dvips filename.ps
%
% to make a filename.tpm which will then be loaded into the document by
% thumbpdf.sty the NEXT time the document is compiled (by pdflatex or
% latex->dvips->ps2pdf). Users must be careful to regenerate the .tpt and/or
% .tpm files if the main document changes and then to recompile the
% document to incorporate the revised thumbnails to ensure that thumbnails
% match the actual pages. It is easy to forget to do this!
%
% Unix systems come with a Perl interpreter. However, MS Windows users
% will usually have to install a Perl interpreter so that the thumbpdf
% script can be run. The Ghostscript PS/PDF interpreter is also required.
% See the thumbpdf docs for details. The latest version and documentation
% can be obtained at.
% http://www.ctan.org/tex-archive/support/thumbpdf/


% NOTE: PDF hyperlink and bookmark features are not required in IEEE
%       papers and their use requires extra complexity and work.
% *** IF USING HYPERREF BE SURE AND CHANGE THE EXAMPLE PDF ***
% *** TITLE/SUBJECT/AUTHOR/KEYWORDS INFO BELOW!!           ***
\newcommand\MYhyperrefoptions{bookmarks=true,bookmarksnumbered=true,
pdfpagemode={UseOutlines},plainpages=false,pdfpagelabels=true,
colorlinks=true,linkcolor={black},citecolor={black},urlcolor={black},
pdftitle={叠前Kirchhoff时间偏移算法（PKTM）在Hadoop 和Spark上的实现},%<!CHANGE!
pdfsubject={Typesetting},%<!CHANGE!
pdfauthor={Yang Chen},%<!CHANGE!
pdfkeywords={Computer Society, IEEEtran, journal, LaTeX, paper,
             template}}%<^!CHANGE!
%\ifCLASSINFOpdf
%\usepackage[\MYhyperrefoptions,pdftex]{hyperref}
%\else
%\usepackage[\MYhyperrefoptions,breaklinks=true,dvips]{hyperref}
%\usepackage{breakurl}
%\fi
% One significant drawback of using hyperref under DVI output is that the
% LaTeX compiler cannot break URLs across lines or pages as can be done
% under pdfLaTeX's PDF output via the hyperref pdftex driver. This is
% probably the single most important capability distinction between the
% DVI and PDF output. Perhaps surprisingly, all the other PDF features
% (PDF bookmarks, thumbnails, etc.) can be preserved in
% .tex->.dvi->.ps->.pdf workflow if the respective packages/scripts are
% loaded/invoked with the correct driver options (dvips, etc.).
% As most IEEE papers use URLs sparingly (mainly in the references), this
% may not be as big an issue as with other publications.
%
% That said, Vilar Camara Neto created his breakurl.sty package which
% permits hyperref to easily break URLs even in dvi mode.
% Note that breakurl, unlike most other packages, must be loaded
% AFTER hyperref. The latest version of breakurl and its documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/breakurl/
% breakurl.sty is not for use under pdflatex pdf mode.
%
% The advanced features offer by hyperref.sty are not required for IEEE
% submission, so users should weigh these features against the added
% complexity of use.
% The package options above demonstrate how to enable PDF bookmarks
% (a type of table of contents viewable in Acrobat Reader) as well as
% PDF document information (title, subject, author and keywords) that is
% viewable in Acrobat reader's Document_Properties menu. PDF document
% information is also used extensively to automate the cataloging of PDF
% documents. The above set of options ensures that hyperlinks will not be
% colored in the text and thus will not be visible in the printed page,
% but will be active on "mouse over". USING COLORS OR OTHER HIGHLIGHTING
% OF HYPERLINKS CAN RESULT IN DOCUMENT REJECTION BY THE IEEE, especially if
% these appear on the "printed" page. IF IN DOUBT, ASK THE RELEVANT
% SUBMISSION EDITOR. You may need to add the option hypertexnames=false if
% you used duplicate equation numbers, etc., but this should not be needed
% in normal IEEE work.
% The latest version of hyperref and its documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/hyperref/


% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\begin{CJK*}{GBK}{song}
%\newcommand{\upcite}[1]{\textsuperscript{\textsuperscript{\cite{#1}}}}
\renewcommand\baselinestretch{1.2}
\renewcommand{\abstractname}{摘要}
\renewcommand{\IEEEkeywordsname}{关键字}
\renewcommand{\figurename}{图}

\begin{document}

\CJKindent
\CJKtilde

%
% paper title
% can use linebreaks \\ within to get better formatting as desired
% Do not put math or special symbols in the title.
\title{叠前Kirchhoff时间偏移（PKTM）算法在Hadoop和Spark上的实现}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethIEEEhowto:kopkaIEEEhowto:kopkaanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{杨晨, 唐杰}

% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%\markboth{Hadoop and Spark framework,~Vol.~11, No.~4, April~2015}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Advanced Demo of IEEEtran.cls for Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2012 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2012 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society journal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
近年来，随着信息技术的飞速发展，各种网络应用带来了数据规模的高速增长，使得大数据迅速发展成为科技界和企业界甚至世界各国政府关注的热点。为了满足海量数据存储和分析需求，需要使用大量计算机协同工作共同完成空前的复杂任务。相较于传统的数据，人们将大数据的特征总结为5个V，即体量大（Volume）、速度快（Velocity）、模态多（Variety）、难辨识（Veracity）和价值密度低（Value）。人类已经进入了大数据时代。Hadoop 作为一个开源的分布式计算系统，具有高容错性、高扩展性和高可靠性，它允许用户在廉价的机器上部署Hadoop集群。随着Hadoop的发展，部署在Hadoop上的程序与日俱增。然而，Hadoop依然有网络传输和磁盘读写等瓶颈，于是又产生了基于内存模型的分布式计算系统――Spark。在本文中，我们首先介绍Hadoop和Spark，以及PKTM 的一些相关知识；然后，我们会介绍下PKTM算法在Hadoop和Spark 框架下的实现；最后，我们对Hadoop和Spark上的PKTM 算法进行实验分析和总结。
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
大数据, Hadoop, Spark, PKTMl, MapReduce, Yarn, RDD.
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when compsoc mode
% is not selected <OR> if conference mode is selected - because compsoc
% conference papers position the abstract like regular (non-compsoc)
% papers do!
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc under a non-conference mode.


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{介绍}
% Computer Society journal papers do something a tad strange with the very
% first section heading (almost always called "Introduction"). They place it
% ABOVE the main text! IEEEtran.cls currently does not do this for you.
% However, You can achieve this effect by making LaTeX jump through some
% hoops via something like:
%
%\ifCLASSOPTIONcompsoc
%  \noindent\raisebox{2\baselineskip}[0pt][0pt]%
%  {\parbox{\columnwidth}{\section{Introduction}\label{sec:introduction}%
%  \global\everypar=\everypar}}%
%  \vspace{-1\baselineskip}\vspace{-\parskip}\par
%\else
%  \section{Introduction}\label{sec:introduction}\par
%\fi
%
% Admittedly, this is a hack and may well be fragile, but seems to do the
% trick for me. Note the need to keep any \label that may be used right
% after \section in the above as the hack puts \section within a raised box.



% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
%
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
%
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by IEEE):
% \IEEEPARstart{A}{}demo file is ....
%
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
%
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{}{\textnormal{}} 随着互联网的飞速发展，特别是近年来随着社交网络、物联网、云计算以及多种传感器的广泛应用，以数量庞大，种类众多，时效性强为特征的非结构化数据不断涌现，数据的重要性愈发凸显，传统的数据存储、分析技术难以实时处理大量的非结构化信息，大数据概念应运而生。虽然计算机存储设备的低廉化以及高速的网络到来在一定程度上减缓了大数据带来的挑战，但数据的增长远远的大于硬件设备的增长，因此传统的大数据处理模型受到了很大的挑战。为了对新时代大数据进行有效的处理，应运而生了一系列大数据分布式处理框架，主要有Hadoop，Spark等。本文主要分为以下几个部分介绍：第二章，我们介绍了一些Hadoop 和Spark分布式处理框架的相关知识；第三章，我们列出了一系列的相关工作；第四章，我们介绍了PKTM 算法在Hadoop和Spark上的实现；第五章，我们在Hadoop和Spark平台上进行一系列实验；最后一章我们对算法进行总结。

% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)

%\hfill mds

%\hfill December 27, 2012

\section{背景}
\par ~~~~在本节中，我们首先在2.1中介绍Hadoop框架；然后在2.2中介绍Spark框架；在2.3节中介绍了Hadoop和Spark对比；最后在2.4中介绍了PKTM算法。
%In this section, we first present the Hadoop framework in 2.1. Then Spark framework shown in 2.2. Finally
%we introduce the PKTM algorithm in 2.3.

\subsection{Hadoop框架}
\par ~~~~Hadoop是一个分布式系统基础架构，由Apache 基金开发。Apache Hadoop是一款支持数据密集型分布式应用并以Apache 2.0许可协议发布的开源软件框架。它支持在商品硬件上构建的大型集群上运行的应用程序。Hadoop 是一个基于Java 语言构建的开源分布式框架，它可以被搭建在廉价的机器上进行各种不同的通用目的海量数据的分析。Hadoop框架透明地为应用提供可靠性和数据移动。它实现了名为MapReduce的编程范式：应用程序被分割成许多小部分，而每个部分都能在集群中的任意节点上执行或重新执行。此外，Hadoop还提供了分布式文件系统，用以存储所有计算节点的数据，这为整个集群带来了非常高的带宽。MapReduce和分布式文件系统HDFS的设计，使得整个框架能够自动处理节点故障，它使应用程序运用成千上万的独立计算的电脑来处理PB级的数据。因此，Apache Hadoop 的核心设计主要是：MapReduce 和HDFS。Hadoop还包括了很多子项目，包括HBase，Hive，Mahout，Sqoop，Zookeeper，\\
Avro等。Hadoop的主要架构图如图1 所示。
\begin{figure}
\renewcommand{\captionfont}{\small}
  \renewcommand{\captionlabelfont}{\small}
  \centering
  \includegraphics[height=7cm,width=8cm,angle=0]{fig0.eps}
  \caption{Hadoop家族架构}
\end{figure}

%Hadoop is an Apache open source distributed computing framework for the cluster is
%stored in inexpensive hardware\cite{BigData}. It's a very popular general purpose
%framework for many different classes of data-intensive applications\cite{HadoopMemory}. The core design of Apache Hadoop\cite{ApacheHadoop}
%is MapReduce and HDFS. MapReduce is a parallel computing framework to run on HDFS, it will
%abstract the user's program for two processes: Map and Reduce. Users only need to write
%Map and Reduce procedures and submitted to the Hadoop framework with the appropriate configuration,
%then it can be run.
%The Hadoop framework will help users to read slice data from HDFS, processing
%Map program, write intermediate results, Shuffle data to Reduce procedures for handling Reduce program,
%and write files to HDFS.
%The program runs on MapReduce framework
%is shown in Fig.1.

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol



\subsubsection{MapReduce}
\par ~~~~MapReduce 是一个最先由Google 提出的分布式计算软件架构，它可支持大数据量的分布式处理。这个架构最初起源于函数式程式的Map 和Reduce两个函数组成，但它们在MapReduce 框架中的应用和原来的使用上大相径庭。MapReduce 框架中，用户的程序总是被分成Map端和Reduce 端，用户只需要填写Map和Reduce 函数，提交应用程序到Hadoop 系统端，系统会自动切分数据，分布式运行提交的程序。Hadoop上MapReduce程序运行流程如图2所示。
\begin{figure}
\renewcommand{\captionfont}{\small}
  \renewcommand{\captionlabelfont}{\small}
\centering
\includegraphics[height=7cm,width=7cm,angle=0]{fig1.eps}
\caption{MapReduce程序流程}
\end{figure}
\par MapReduce是一个高性能的批处理分布式计算框架，用于对海量数据进行并行分析和处理。与传统的数据仓库和分析技术相比，MapReduce 适合处理各种类型的数据，包括结构化、半结构化和非结构化数据。数据量在TB和PB级别，在这个量级上，传统方法通常已经无法处理数据。MapReduce 将分析任务分为大量的并行Map任务和Reduce任务两类。Map和Reduce任务可以运行在多个服务器上。MapReduce适合处理的任务：
\begin{enumerate}[(1)]
\setlength{\itemindent}{2em}
\item 复杂的数据：业务数据不能适合行列的数据结构。数据可能来源于多种格式：多媒体数据、图像数据、文本数据、实时数据、传感器数据等等。当有新的数据来源时，可能会有新的数据格式的出现。MapReduce可以存放和分析各种原始数据格式。
\item 超大规模数据：很多公司仅仅因为数据存放成本过高就放弃了很多有价值的数据。新的数据来源使得问题更为严重，新的系统和用户带来比以往更多的数据。Hadoop的创新构架使用低成本的常规服务器存储和处理海量的数据。
\item 新的分析手段：海量复杂数据分析需要使用新的方法。新的算法包括自然语言分析、模式识别等。只有Hadoop的架构才能方便高效地使用新的算法来处理和分析海量数据。
\end{enumerate}
\par MapReduce框架的核心优势：
\begin{enumerate}[a)]
\setlength{\itemindent}{2em}
\item 高度可扩展，可动态增加/削减计算节点，真正实现弹性计算。
\item 高容错能力，支持任务自动迁移、重试和预测执行，不受计算节点故障影响。
\item 公平调度算法，支持优先级和任务抢占，兼顾长/短任务，有效支持交互任务。
\item 就近调度算法，调度任务到最近的数据节点，有效降低网络带宽。
\item 动态灵活的资源分配和调度，达到资源利用最大化，计算节点出现闲置和过载的情况；同时支持资源配额管理。
\end{enumerate}
%MapReduce is a programming model and an associated implementation for processing and generating large dataset
%that is amenable to a broad variety of read-world tasks\cite{MapReduce}. A key benefit of MapReduce is that it automatically handles failures, hiding the complexity
%of fault-tolerance from the programmer\cite{MapReduce2}.

\par 目前MapReduce主要有两个版本：MRv1 和MRv2（Yarn）。


\par MRv1是第一代MapReduce计算框架。它由两部分组成：编程模型和运行时环境。它的基本编程模型是将问题抽象成Map和Reduce两个阶段。其中，Map阶段将输入数据解析成\emph{$<key,value>$}键值对形式，并行调用map处理后，再以\emph{$<key,value>$}的形式输出Reduce节点进行处理；Reduce 阶段则将key 相同的value进行归约处理，并将最终的结果写到HDFS 上。MRv1的核心功能主要是：JobTracker和TaskTracker。MRv1的框架结构如图3所示.
\begin{enumerate}[1)]
\setlength{\itemindent}{2em}
\item JobTracker是整个MapReduce计算框架中的主服务，相当于集群的“管理者”，负责整个集群的作业控制和资源管理。在Hadoop内部，每个应用程序被表示成一个作业，每个作业又被进一步分成多个任务，而JobTracker的作业控制模块则负责作业的分解和状态监控。其中，最重要的是状态监控，主要包括TaskTracker的状态监控、作业状态监控和任务状态监控等。其主要作用有两个：容错和为任务调度提供决策依据。一方面，通过状态监控，JobTracker 能够及时发现存在异常或者出现故障的TaskTracker、作业或者任务，从而启动相应的容错机制进行处理；另一方面，由于JobTracker 保存了作业和任务的近似实时运行信息，这些可用于任务调度时进行任务选择的依据。资源管理模块的作用是通过一定的策略将各个节点上的计算资源分配给集群中的任务。它由可插拔的任务调度器的地方，用户可根据自己的需要编写相应的调度器。
    \par ~~JobTracker是一个后台服务进程，启动之后，会一直监听并接收来自各个TaskTracker发送的心跳信息，这里面包含节点资源使用情况和任务运行情况等信息。JobTracker会将这些信息统一保存起来，并根据需要为TaskTracker分配新任务。JobTracker的主要功能就是作业控制和资源管理。
\item TaskTracker是Hadoop集群中运行于各个节点上的服务，它扮演着“通信枢纽”的角色，是JobTracker与Task之间的“沟通桥梁”：一方面，它从JobTracker 端接收并执行各种命令，比如运行任务、提交任务、杀死任务等；另一方面，它将本节点上的各个任务状态通过周期性心跳汇报给JobTracker。TaskTracker与JobTracker 和Task 之间采用了RPC协议进行通信。对于TaskTracker和JobTracker而言，它们之间采用InterTrackerProtocol 协议，其中，JobTracker扮演RPC Server 的角色，而TaskTracker 扮演RPC Client 角色；对于TaskTracker与Task而言，它们之间采用TaskUmbilicalProtocol协议，其中，TaskTracker扮演RPC Server 的角色，而Task扮演RPC Client的角色。TaskTracker主要实现了两个功能：汇报心跳和执行命令。

\end{enumerate}
%MRv1(MapReduce version1) is implemented in
%Hadoop1.0, which consists of three parts: the programming model(Programming API), the Runtime environment(JobTracker
%and TaskTracker) and Data Engine(MapTask and ReduceTask).
%JobTracker is primarily responsible for job control and resource management, thus cause a heavy burden.
%TaskTrackers actively communicates with JobTracker, receives jobs
%and is directly responsible for the running of each task.

\begin{figure}
\renewcommand{\captionfont}{\small}
  \renewcommand{\captionlabelfont}{\small}
\centering
\includegraphics[height=6cm,width=7cm,angle=0]{fig2.eps}
\caption{MRv1框架}
\end{figure}

\par 随着数据量的高速增长和新型应用的出现，MRv1 在扩展性、可靠性、资源利用率和多框架支持等方面暴露出了明显的不足，由此诞生了下一代MapReduce 框架（MRv2）。MRv1的主要问题有以下几个方面：
\begin{enumerate}[1)]
\setlength{\itemindent}{2em}
\item 扩展性差：在MRv1中，JobTracker同时具备了资源管理和作业控制两个功能，这成为系统的一个最大的瓶颈，严重制约了Hadoop集群的扩展。
\item 可靠性差：MRv1采用了master/slave 结构，其中，master存在单点故障问题，一旦它出现故障，将导致整个集群不可用。
\item 资源利用率低：MRv1采用了基于槽位的资源分配模型。槽位是一种粗粒度的资源划分单位，通常一个任务不会用完槽位对应的资源，且其他任务也无法使用这些空闲资源。此外，Hadoop将槽位分为Map slot和Reduce slot两种，且不允许它们之间共享，这常常会导致一种槽位资源紧张而另外一种闲置的情况出现（比如一个作业刚刚提交时，只会运行Map Task，此时Reduce slot闲置）。
\item 无法支持多种计算框架：随着互联网的高速发展，MapReduce这种基于磁盘的离线计算框架已经不能满足应用要求，从而出现了一些新的计算框架，包括内存计算框架、流式计算框架和迭代式计算框架等，而MRv1不能支持多种计算框架并存。
\end{enumerate}
\par ~~下一代MapReduce框架的基本设计思想是将JobTracker的两个主要功能，即资源管理和作业控制（包括作业监控、容错等），分拆成两个独立的进程：资源管理进程和作业控制进程。资源管理进程是与具体应用程序无关的模块，它负责整个集群的资源（内存、CPU、磁盘等）管理；而作业控制进程是直接与应用程序相关的模块，且每个作业控制进程只负责管理一个作业。这样，通过将原有JobTracker中与应用程序相关和无关的模块分开，不仅减轻了JobTracker 负载，也使得Hadoop支持更多的计算框架。
\par ~~随着互联网的高速发展，基于数据密集型应用的计算框架不断出现。从支持离线处理的MapReduce，到支持在线处理的Storm，从迭代计算框架Spark 到流式处理框架S4....各种框架诞生于不同的公司或者实验室。它们各有所长，各自解决了某一类应用问题，而在大部分互联网公司中，这几种框架可能同时被采用。公司一般希望将所有这些框架部署到一个公共的资源中，让它们共享集群的资源，这就产生了资源统一管理与调度平台的典型代表Apache Yarn。
\par ~~Yarn是Apache的下一代MapReduce框架，它的基本设计思想是将JobTracker拆分成两个独立的服务：一个全局的资源管理器ResourceManager负责对各个NodeManager 上的资源进行统一管理和调度。当用户提交一个应用程序时，需要提供一个用于跟踪和管理这个程序的ApplicationMaster。 它负责向ResourceManager 申请资源，并要求NodeManager启动可以占用一定资源的任务。由于不同的ApplicationMaster分布在不同的节点上，因此它们之间不会相互影响。Yarn主要由三部分组成：ResourceManager、NodeManager、\\ ApplicationMaster。Yarn的结构如图4所示。
\begin{enumerate}[1)]
\setlength{\itemindent}{2em}
\item ResourceManager是一个全局的资源管理器，负责整个系统的资源管理和分配。它主要由两个组件构成：调度器（Scheduler）和应用管理器（ASM）。调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。该调度器只是个单纯的调度器，不再从事任何与应用程序相关的工作。调度器仅根据各个应用的资源需求进行资源分配，而资源分配单位用一个抽象概念“资源容器”（Container）表示。Container是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。调度器是一个可插拔的组件，用户可以根据自己的需要设计新的调度器；应用程序管理器（ASM）负责整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。
\item NodeManager是每个节点上的资源和任务管理器。一方面，它会定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态；另一方面，它会接收并处理来自AM的任务启动/停止等各种请求。
\item 用户提交的每个应用程序均包含一个AM。它实际上是一个简化版的JobTracker，主要功能包括：与RM 调度器协商以获取资源、与NM通信以启动/停止任务、监控所有任务的运行状态，并在任务运行失败时重新为任务申请资源以重启任务。
\end{enumerate}
%Due to the heavy burden of the JobTracker, it produced a new framework(MRv2) also called Yarn which mainly
%abstracted the resouce management of the JobTracker. Yarn is a new architecture that decouples the programming
%model from the resource management infrastructure, and delegates many scheduling functions(e.g.,task fault-tolerance)
%to per-application components\cite{Yarn1}. It consists of
%four basic components: ResourceManager, ApplicationMaster, NodeManager, Container.
%ResourceManager is responsible for the management and allocation of the system resources. ApplicationMaster communicates to the ResourceManager to get the resources. NodeManager schedules resources and tasks on each node. Container is Yarn abstract concept of
%resources, while it is also the basic unit of system resource allocation.
%Yarn's structure shown in Fig.3.

\begin{figure}
\renewcommand{\captionfont}{\small}
  \renewcommand{\captionlabelfont}{\small}
\centering
\includegraphics[height=6cm,width=7cm,angle=0]{fig3.eps}
\caption{Yarn架构}
\end{figure}


\subsubsection{HDFS}
\par ~~~~HDFS （Hadoop Distributed File System）是Apache Hadoop 项目的一个子项目，是一个高度容错的分布式文件系统，设计用于在低成本硬件上运行。HDFS 提供高吞吐量应用程序数据访问功能，适合带有大型数据集的应用程序。HDFS作为一个分布式文件系统，具有高容错的特点，它可以部署在廉价的通用硬件上，提供高吞吐率的数据访问，适合那些需要处理海量数据集的应用程序。HDFS 没有遵循可移植操作系统接口（POSIX）要求，不支持“ls”或“cp”这样的标准UNIX命令，也不支持如fopen和fread这样的文件读写方法，二是提供了一套特有的、基于Hadoop抽象文件系统的API，支持以流的形式访问文件系统的数据。HDFS的主要特性包括：
\begin{enumerate}[(1)]
\setlength{\itemindent}{2em}
\item 支持超大文件。超大文件在这里指的是几百MB，几百GB甚至几百TB大小的文件，一般来说，一个Hadoop文件系统会存储T、P级别的数据。Hadoop需要能够支持这种级别的大文件。
\item 检测和快速应对硬件故障。在大量通用硬件平台上构建集群时，故障，特别是硬件故障是常见的问题。一般的HDFS系统是由数百台甚至上千台存储着数据文件的服务器组成，这么多的服务器意味着高故障率。因此故障检测和自动恢复是HDFS的一个设计目标。
\item 流式数据访问。HDFS处理的数据规模都比较大，应用一次需要访问大量的数据。同时，这些应用一般是批量处理，而不是用户交互式处理。HDFS 使应用程序能够以流的形式访问数据集，注重的是数据的吞吐量，而不是数据访问的速度。
\item 简化的一致模型。大部分的HDFS程序操作文件时需要一次写入，多次读取。在HDFS中，一个文件一旦经过创建、写入、关闭后，一般就不需要修改。这样简单的一致性模型，有利于提供高吞吐量的数据访问模型。
\end{enumerate}
\par 为了支持流式数据访问和存储超大文件，HDFS引入了一些比较特殊的设计，在一个全配置的集群上，“运行HDFS”意味着在网络分布的不同服务器上运行一些守护进程，这些进程有各自的特殊角色，并相互配合，一起形成一个分布式文件系统。HDFS采用了主从体系结构，名字节点NameNode、 数据节点DataNode和客户端Client 是HDFS中3个重要的角色。NameNode是HDFS主从结构中主节点上运行的主要进程，它指导主从结构中的从节点，DataNode执行底层的I/O任务。NameNode 管理着文件系统的Namespace。它维护着文件系统树以及文件树中所有文件和文件夹的元数据。管理这些信息的文件有两个，分别是Namespace镜像文件和操作日志文件，这些信息被Cache 在RAM中，当然，这两个文件也会被持久化存储在本地硬盘。NameNode记录着每个文件中各个块所在的数据节点的位置信息，但是他并不持久化存储这些信息，因为这些信息会在系统启动时从数据节点重建；DataNode是文件系统的工作节点，他们根据客户端或者NameNode 的调度存储和检索数据，并且定期向NameNode发送他们所存储的块的列表；Client 代表用户与NameNode和DataNode交互来访问整个文件系统。Client提供了一些列的文件系统接口，因此我们在编程时，几乎无须知道DataNode 和NameNode，即可完成我们所需要的功能。HDFS的结构图如图5所示。
%HDFS(Hadoop Distributed File System) which comes from the Google File System\cite{HDFS1} as a storage system that allow you to connect multiple nodes in a cluster which
%distributes some of data files. HDFS is a "write once, read many" model. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS is suitable for applications that have large data sets\cite{HDFS2}. HDFS has two main components: NameNode and DataNode. HDFS's structure shown in Fig.4.

\begin{figure}
\renewcommand{\captionfont}{\small}
  \renewcommand{\captionlabelfont}{\small}
\centering
\includegraphics[height=6cm,width=7cm,angle=0]{fig4.eps}
\caption{HDFS结构}
\end{figure}

\subsection{Spark框架}
\par ~~~~Spark是UC Berkeley AMP lab所开源的类Hadoop MapReduce的通用的并行计算框架，Spark基于Map Reduce 算法实现的分布式计算，拥有Hadoop MapReduce所具有的优点；但不同于MapReduce的是Job中间输出的结果可以保存在内存中，从而不再需要读写HDFS，因此Spark 能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce 的算法。Spark的核心组件是RDD（Resilient Distributed Datasets）。Spark的架构图如图6所示。
\begin{figure}
\renewcommand{\captionfont}{\small}
\renewcommand{\captionlabelfont}{\small}
\centering
\includegraphics[height=6cm,width=8cm,angle=0]{fig60.eps}
\caption{Spark架构}
\end{figure}

%Apache Spark which developed by UCBerkeley's AMP laboratory is a fast and general-purpose cluster computing system. It
%provides high-level APIs in Java, Scala and Python, and an optimized engine that supports general execution graphs\cite{Spark1}.
%Spark is a MapReduce-like cluster computing framework\cite{Spark1,Spark2}. But there are some differences between Spark and Hadoop, Spark enables memory distributed data sets, provides interactive query, optimize
%iterative workloads.
%Spark is implemented in the Scala language, it uses Scala be an application framework. Compared to Hadoop,
%Spark and Scala can be tightly integrated, Scala can operate a distributed data sets as easily as operating a local collection of
%objects. Spark is designed for a specify type of cluster computing workloads, namely those between the parallel operation to reuse
%the workload of the data sets.
%In order to optimize the interative types of workloads, Spark has introduced the concept of memory computing(RDD),
%which data sets can be cached in the memory to shorten the access latency.

\subsubsection{RDD弹性分布式数据集}
\par ~~~~RDD是一个容错的、并行的数据结构，可以让用户显式地将数据存储到磁盘和内存中，并能控制数据的分区。同时，RDD还提供了一组丰富的操作来操作这些数据。RDD作为数据结构，本质上是一个只读的分区记录集合。一个RDD可以包含多个分区，每个分区就是一个dataset片段。RDD可以相互依赖。如果RDD的每个分区最多只能被一个Child RDD的一个分区使用，则称之为narrow dependency；若多个Child RDD分区都可以依赖，则称之为wide dependency。不同的操作依据其特性，可能会产生不同的依赖。RDD的依赖模型如图7所示。Spark之所以将依赖分为narrow与wide，基于两点原因。首先，narrow dependencies可以支持在同一个cluster node上以管道形式执行多条命令；其次，则是从失败恢复的角度考虑，narrow dependencies 的失败恢复更有效，因为它只需要重新计算丢失的parent partition即可，而且可以并行地在不同节点进行重计算，而wide dependencies牵涉到RDD各级的多个parent partitions。
%A RDD(Resilient Distributed Data) is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost\cite{Spark2}.
% To achieve fault tolerance efficiently, RDDs provide a restricted form of shared
%memory, based on coarsegrained transformations rather than fine-grained updates to shared state\cite{Spark3}.
%\par RDD can interdependence.
%It's useful to classify dependencies into two types: narrow dependencies, where each partition of the parent RDD is used by at most
%one partition of the child RDD; wide dependencies, where multiple child partitions may depend on it\cite{Spark3}. Although Spark can achieve
% tremendous speedup, we suspect the memory cost it has to pay for the introduction of RDDs\cite{HadoopMemory}. RDD's dependencies shown in Fig.5.

\begin{figure}
\renewcommand{\captionfont}{\small}
  \renewcommand{\captionlabelfont}{\small}
\centering
\includegraphics[height=6cm,width=8cm,angle=0]{fig5.eps}
\caption{RDD dependencies}
\end{figure}

\par RDD本质上是一个内存数据集，在访问RDD时，指针只会指向与操作有关的部分。RDD将操作分为两类：transformation和action。无论执行多少次transformation 操作，RDD都不会真正执行运算，只有当action 操作被执行时，运算才会触发。而在RDD内部实现机制中，底层接口则是基于迭代器的，从而使得数据访问变得更高效，也避免了大量中间结果对内存的消耗。
\par RDD是Spark最核心的东西，它表示已被分区、不可变的并能够被并行操作的数据集合，不同的数据集格式对应不同的RDD实现。RDD必须是可序列化的。RDD 可以cache到内存中，每次对RDD数据集的操作之后的结构，都可以存放到内存中，下一个操作可以直接从内存中输入，省去了MapReduce大量的磁盘I/O操作。这对迭代运算比较常见的机器学习算法，交互式数据挖掘来说，效率大大提升。总结来说，RDD有一下几个特点：
\begin{enumerate}[(1)]
\setlength{\itemindent}{2em}
\item 它是在集群节点上的不可变的，已分区的集合对象。
\item 通过并行转换的方式来创建如（map，filter，join，etc）。
\item 失败自动重建。
\item 可以控制存储级别（内存、磁盘等）来进行重用。
\item 必须是可序列化的。
\item 是静态类型的。
\end{enumerate}
\par RDD的生成有两种创建方式：
\begin{enumerate}[(1)]
\setlength{\itemindent}{2em}
\item 从Hadoop文件系统（或与Hadoop兼容的其他存储系统）输入（例如HDFS）创建。
\item 从父RDD转换得到新RDD。
\end{enumerate}

\subsection{Hadoop vs Spark}
\par ~~~~Spark与Hadoop的对比：尽管Hadoop适合大多数批处理工作负载，而且在大数据时代成为企业的首选技术，但由于以下几个限制，它对一些工作负载并不是最优选择：缺少对迭代的支持；需要将中间数据存在硬盘上以保持一致性，因此会有比较高的延迟。在Spark集群中，有两个重要的元素，即driver和worker。driver程序是应用逻辑执行的起点，而多个worker用来对数据进行并行处理。尽管不是强制的，但数据通常是与worker搭配，并在集群内的同一套机器中进行分区。在执行阶段，driver程序会将code/closure传递给worker机器，同时相应分区的数据将进行处理。数据会经历转换的各个阶段，同时尽可能地保持在同一分区之内。执行结束后，worker会将结果返回到driver程序。Spark上driver和worker运行模式如图8所示。总的来说Hadoop和Spark对比有一下几个方面：
\begin{enumerate}[(1)]
\setlength{\itemindent}{2em}
\item Spark的中间数据放到内存中，对于迭代运算效率更高。Spark更适合于迭代运算比较多的ML 和DM运算，因为在Spark中，有RDD概念。
\item Spark比Hadoop更通用。Spark提供的数据集操作类型有很多种，不像Hadoop只提供Map和Reduce 两种操作。然而由于RDD特性，Spark不适用那种异步细粒度更新状态的应用。
\item 容错性。在分布式数据集计算时通过checkpoint 来实现容错，而checkpoint 有两种方式，一个是checkpoint data，一个是logging the updates。 用户可以控制采用哪种方式来实现容错。
\item 可用性。Spark通过提供丰富的Scala，Java，Python及交互式Shell来提高可用性。
\end{enumerate}

\begin{figure}
\renewcommand{\captionfont}{\small}
  \renewcommand{\captionlabelfont}{\small}
\centering
\includegraphics[height=6cm,width=8cm,angle=0]{fig5.eps}
\caption{RDD dependencies}
\end{figure}

\subsection{Kirchhoff}
\par ~~~~Kirchhoff叠前时间偏移是地震数据处理中最耗时的常用模块之一。Kirchhoff 叠前时间偏移假定已知地震道数据\emph{$t_0$}，对于所有的反射点，从激发点到接收点的传输时间\emph{$T_{SR}=t_0$}，已知地震道数据的激发点和接收点的坐标，在传输时间\emph{$T_{SR}$}内，反射点的轨迹如图六所示。利用叠前时间偏移可以有效地解决交叉地层速度的矛盾性问题，通过比较图中常规速度谱和叠前时间偏移速度谱，可以看到经过处理后，速度谱能量更加集中，速度拾取矛盾也可以得到有效解决。
\begin{figure}
\renewcommand{\captionfont}{\small}
  \renewcommand{\captionlabelfont}{\small}
\centering
\includegraphics[height=4cm,width=7cm,angle=0]{kirchhoff.eps}
\caption{地震成像示意图}
\end{figure}
\begin{algorithm}
\caption{Kirchhoff算法}
\begin{algorithmic}[1]
\Procedure{Kirchhoff}{inputtraces}
\ForAll{input traces}
\State read input trace
\State filter input trace
\ForAll{output traces within aperture}
\ForAll{output trace contributed samples}
\State compute travel time
\State compute amplitude correction
\State select input sample and filter
\State accumulate input sample contribution into output sample
\EndFor
\EndFor
\EndFor
\State dump output volume
\EndProcedure
\end{algorithmic}
\end{algorithm}
Kirchhoff算法流程和文件处理流程如图七所示。
%In general, seismic data processing is divided into two steps in practice. The first step applies signal
%processing algorithms to normalize the signal over the entire survey and to increase the signal-to-noise ratio. Hundreds of mathematical algorithms are available during this step, from which geophysical
%specialist will select the particular candidates for seismic data by experience. The second step, which is
%the most time-consuming one, is designed to correct for the effects of changing subsurface media
%velocity on the wave propagation through the earth. Pre-Stack Kirchhoff Time Migration (PKTM)
%algorithm used in the second step is one of the most widely adopted imaging methods in the seismic
% data processing industry\cite{PKTM}.
%The oil and gas industries have been great consumers of parallel and distributed computing systems, by frequently running technical
%applications with intensive processing of terabytes of data. Since drilling each oil/gas well in exploration areas costs several tens of
%millions of dollars, producing high-quality seismic images in reasonable time can notably decrease the risk of drilling a "dry hole"\cite{Kirchhoff}.
%PKTM is one of the most popular imaging approaches in the seismic data processing industry\cite{GPU}. The PKTM algorithm and program data flow structure shown
%in Fig.6.

%\begin{figure}[H]
%\renewcommand{\captionfont}{\small}
%  \renewcommand{\captionlabelfont}{\small}
%\centering
%\includegraphics[height=7cm,width=6cm,angle=0]{fig6.eps}
%\caption{Kirchhoff on CPU}
%\end{figure}

\begin{figure}
\renewcommand{\captionfont}{\small}
  \renewcommand{\captionlabelfont}{\small}
\centering
\includegraphics[height=6cm,width=3cm,angle=0]{fig6.eps}
\includegraphics[height=5cm,width=5cm,angle=0]{fig71.eps}
\caption{PKTM}
\end{figure}


%\subsection{Hadoop vs Spark}
%\par ~~~~The differences between Spark and Hadoop are as follows:
%
%\par \setlength{\parindent}{2em}(1) Fast: Spark persists the intermediate data into memory, so it's more efficient for high iterations program.
%Spark is more suitable for more iterations of ML and DM operations, because Spark has the abstraction RDD.
%
%\par \setlength{\parindent}{2em}(2) Flexible: Spark provides many types of dataset operation, rather than Hadoop which only provides two types
%of dataset operations---Map and Reduce. These operations include: map, filter, flatMap, sample, groupByKey, reduceByKey, union and so on which
%be called Transformation, also include count, collect, reduce, save and so on that be called action. These diverse types of data collection
%operations give the convenience to the users who want to develop the upper application. Spark's communication models between the various processing
%nodes which does not like Hadoop's only model---Data Shuffle, have many types. Users can name, persist, control intermediate results' storage and
%partitions. It can be said that Spark programming model is more flexible than Hadoop.
%
%\par \setlength{\parindent}{2em}(3) Fault Tolerance: In RDD calculations, it performs fault tolerance by checkpoint. There are two ways to do
%the checkpoint: checkpoint data and logging the updates. Users can control which way to achieve fault tolerance, the default mode is logging
%the updates, keep tracking of all records generated by RDD conversion(transformation), recording lineage of each RDD to recalculate the data
%to generate lost partitions.

\section{Related Work}
\par ~~~~目前已经有一些Kirchhoff算法并行计算的相关工作：首先我们介绍PKTM在MapReduce上的实现；然后介绍PKTM在GPU上的实现。
\begin{enumerate}[(1)]
\setlength{\itemindent}{2em}
\item MapReduce：这篇文章\cite{Parallel}介绍了PKTM在Hadoop上的实现，它按照输入道划分mapper，每个mapper包含一系列输入道数据，然后计算输出道，shuffle 数据，并把输出道数据发送到reduce端，在reduce端合并输出道，并把结果输出到HDFS。该程序没有考虑集群的环境，当集群环境变化时，程序需要做很大的改动。还有篇文章\cite{KirchhoffMapReduce}同样介绍了PKTM运行在MapReduce框架下，该程序使用了同样的划分方法划分输入道，然后在mapper中计算，并把结果发送到reduce端，但是该程序没有考虑网络带宽的问题，发送的中间数据特别巨大，大大的影响了程序的性能。总的来说，MapReduce上的PKTM实现，主要的问题有三个方面：
    \begin{enumerate}[a)]
    \setlength{\itemindent}{2em}
    \item 应对数据量的不同，性能变化很大。一些程序在小量的数据上运行性能很好，但是在大数据集上运行性能会变的很差；还有就是随着集群机器的扩展，性能没有稳步的提升。
    \item 网络传输量的问题。当数据量变大时，不可避免的一个问题是中间数据量的增大，这就给网络传输带来了一定的负担。不同节点之间的传输性能会严重影响程序的性能。
    \item HDFS读写问题。程序的数据保存在HDFS上，这样就保证了所有节点可以共享所有数据，HDFS还提供了强大的容错能力，可以帮助恢复失败的任务，但是HDFS的读写性能有一定的瓶颈，不支持并行读写，因此解决了HDFS的读写问题可以大大的提高程序的性能。
    \end{enumerate}
\item GPU：这篇文章\cite{PKTM}介绍了PKTM在GPGPU上的实现，它用multi-GPUs来计算数据，该程序大大加速了PKTM算法相对于CPU算法。主要是利用GPU的成百上千的线程来计算数据。大大的提高了计算的并行性。但当数据量特别大时，GPU的显存容量很小，GPU和CPU就需要不断的来交换数据，以及CPU不断的读取数据。这是GPU的一个瓶颈。另一篇文章\cite{KirchhoffAccelerating}也是介绍了GPU上的PKTM的实现，它提供了大概20倍的加速相对于CPU计算，但是它的CPU I/O读写和循环控制过载。总的来说，GPU上的PKTM实现主要有一下几方面问题：
    \begin{enumerate}[a)]
    \setlength{\itemindent}{2em}
    \item 显存容量问题。虽然GPU有成百上千的线程用于计算数据，但是GPU的显存容量太小，是它的一个瓶颈，计算时需要等待数据从内存交换到显存，在大数据的情况下这就大大的影响了程序的性能。
    \item GPU同步问题。在多个GPU的情况下，GPU的同步显然就是个严重的问题，如果单纯的只使用一个GPU来加速，由于显存容量以及系统的资源问题，加速的效果大大的弱化，因此使用多GPU加速，GPU之间的同步成为了程序的一个瓶颈，怎样很好的去同步计算的数据依然是现在的一个问题。
    \item 任务失败重启。目前已有的分布式集群计算框架中对GPU任务的管理不够完善，几乎没有GPU任务失败重启的功能，也就是说当数据量特别大时，总会有GPU任务运行失败，就会导致整个程序的失败，这是个很严重的瓶颈问题。
    \end{enumerate}
\end{enumerate}
%Before I do this program, there are several papers that introduce Kirchhoff on MapReduce framework. Firstly, we introduce the implementation
%of PKTM with MapReduce framework. This paper\cite{Parallel} introduces PKTM on MapReduce framework, it just simply cuts the input traces into several splits and sends these split to the corresponding mappers, then computing the data, shuffle data to the reduce task, write output to the HDFS. It does not consider the environment of the cluster, if the cluster environment changed, its program will run with problems. This paper\cite{KirchhoffMapReduce} also shows the PKTM on MapReduce. It splits data into traces, sends traces to Map function, then computes traces, shuffles data to Reduce function. The program use MapReduce parallel framework to realize parallelism of the PKTM, but the problem is that the shuffle data is very huge, it has seriously affected the performance of the program. The type of MapReduce methods all have the bottleneck of I/O time cross computing.
%\par Secondly, we describe the use of GPUs to achieve parallelism. This paper\cite{PKTM} describes the PKTM on GPGPU\cite{GPGPU}, it use multi-GPUs to compute the data. It takes the great advantage of the CPU implementation. But it is over-reliance on GPU and when the data gets bigger and bigger, the memory on GPU will be the bottleneck of the program. Another PKTM on GPU is in this paper\cite{KirchhoffAccelerating}. It's execution is 20 times faster than a pure CPU execution, but its GPU I/O and loop control is overloading. Overall, the type of GPUs methods all have the problem of data synchronize between GPUs and small memory capacity of GPUs.
%\par Finally, we show the combination of the GPUs and MapReduce. This paper\cite{GPUMapReduce} shows the MapReduce framework on GPUs, it can greatly accelerate the execution time, but it just abstract the Map function to a GPU function statically. If the Map function needs to change, the whole GPU
%codes must change. This does not benefit to expansion.

\section{算法设计与实现}


\subsection{Hadoop上的Kirchhoff算法实现}
\par ~~~~Hadoop算法的设计主要是Map和Reduce的过程，算法的主要流程如下所示：
%Hadoop's algorithm design is primarily the MapReduce process. The algorithm flow is as follows:

\begin{enumerate}[1)]
\setlength{\itemindent}{2em}
\item 分析系统环境：根据Hadoop的官方介绍，一个节点上的mapper数在10$\sim$100之间的效果是最好的，我们可以获得集群的环境配置，如：集群节点个数\emph{n}，每个节点上的内存总量\emph{{$M_1$}, {$M_2$}... {$M_n$}}，CPU的总数\emph{cpus}，每个CPU的核数为\emph{cores}，每个核的线程数为\emph{threads}。
%Get Cluster Environment: According to Hadoop\cite{ApacheHadoop}, one node can perform well when their mappers are between 10 and 100. Through the detection of the cluster, we get the number of nodes \emph{n} and in each node, we get the memory \emph{{$M_1$}, {$M_2$}... {$M_n$}}, CPUs \emph{cpus}, cores per CPU \emph{cores}, threads per core \emph{threads}.

\item 自定义输入阶段：在Hadoop中，每个输入文件块对应一个mapper。Hadoop提供了一个默认的分块机制，64M大小对应一块，不足64M的文件也默认是一块。我们通过中心点文件对输入道进行划分，由于中心点文件大小比较小，所以我们要自定义划分mapper的数量。Hadoop提供了重写
    \emph{FileInputFormat}类来进行逻辑划分输入文件为split，每个split都对应一个mapper。因此我们只要设置了每个split的大小就可以确定mapper 的数量。首先在\emph{FileInputFormat}中获取所有split的总大小：\emph{$S_1$}, \emph{$S_2$}, \emph{$S_3$}... \emph{$S_n$}--\emph{$\sum_{i=1}^n S_i$}。在Hadoop 中，Reduce任务的个数是由用户设置的\emph{$r_n$}，在mapper任务没有结束时，reduce任务就会被启动，因此，reduce占用的资源会影响mapper的数量，基于这些考虑设置split 大小：\emph{\[f_{split}=\frac{\left(\sum_{i=1}^n S_i\right)}{k*min\left(\left(cpus*cores+r_n\right),\left(\frac{\sum_{i=1}^n M_i}{M}\right)\right)}\]} \\
     \emph{k$\in{[1,\infty)}$}是一个可控参数，保证mapper的数量是集群资源容纳mapper数量的倍数，这样每次mappers都可以并行执行，不会剩余很少的mappers最后运行。最后我们通过设置\emph{\small{mapreduce.input.fileinputformat.split.maxsize}} 为\emph{$f_{split}$}来划分输入文件。另一个限制是，保证split的大小为一道数据大小的倍数，防止切分一道数据。通过输入split，我们读取输入道数据，形成$<key,value>$ 键值对，并发送到每个mapper 中进行处理。\emph{key} 代表输入道在文件中的道号，\emph{value}代表每个输入道对应的中心点坐标。
%Input Phase: In Hadoop, each input files block correspond to a mapper. So, in order to control numbers of mappers, we override the \emph{FileInputFormat} which provided methods to logically split the input files, each split produces a mapper. We only need to
%    decide the size of the split. Firstly, we get all the splits
%    lengths \emph{$S_1$}, \emph{$S_2$}, \emph{$S_3$}... \emph{$S_n$}. Based on these lengths, we get the total size of the input files as \emph{$\sum_{i=1}^n S_i$}.
    %In Hadoop, the number of reduce tasks is set by the user, we can get the number \emph{$r_n$}. Then because of the mappers between 10$\sim$100, the size of split is as follows:\emph{\[f_{split}=\frac{\left(\sum_{i=1}^n S_i\right)}{k*min\left(\left(cpus*cores+r_n\right),\left(\frac{\sum_{i=1}^n M_i}{M}\right)\right)}\]} \\
%     \emph{k$\in{[1,\infty)}$} is a controllable parameter that can be modified. But \emph{$f_{split}$} must greater than 0. \emph{M} is the memory size of a mapper. Finally we set the \emph{\small{mapreduce.input.fileinputformat.split.maxsize}} as \emph{$f_{split}$} to decide the split size. Through input split, we read the coordinate points of the input trace, producing $<key,value>$ pairs. \emph{key} represents the offset of input trace in
%    the input files, \emph{value} represents the coordinate points of each input trace. One pair may combine many input traces. These pairs will be submitted to the map task for execution.

\item Map阶段：对split传过来的输入道数据进行处理，读取HDFS上的速度数据，炮点坐标数据和检波器坐标数据，每个mapper都是并行的进行处理，这加快了运算的速度。由于每个输入道都会产生一定量的输出道，每个不同的输入道产生的输出道有很多重合，因此，我们再mapper中用一个HashMap保存输出道的数据，相同的输出道被叠加到同一个value上，节省了数据传输时间。另一点是，每个输入道产生输出道会进行多次循环，大大的浪费了时间，因此在每个mapper 中我们用多线程来处理输入道数据，我们设置每个mapper数据的线程数为
    \emph{$\left(threads-2\right)$}，如果系统支持超线程，则设置每个mapper中的线程数为\emph{$2*\left(threads-2\right)$}。最后一个策略是将输出道的数据写入本地文件，减少数据的发送量，减轻网络带宽。将一个mapper中的输出道写入二进制文件，保存输出道的道号和文件名、偏移量，格式如：\emph{$<key, filename\#offset>$}，只发送这些键值对到Reduce 任务。
%Map Phase: Since the input files are logically divided into \emph{E} splits, each split data is submitted to a map task, these map tasks will be computing in
%parallel. Each map task computes a lot of input traces, and each input trace will produce many output traces, so in order to achieve parallelism
%within the map task, we use the multi-threads to process input traces. Each thread shares a data structure HashMap which used to save the output
%trace, so that the same output trace will be merge locally which reduce the load of the data transmission. we detect the \emph{Hyper-Threading} of the cluster systems. If it is on, we set the number threads of the mapper is \emph{$2*\left(threads-2\right)$}, else it is \emph{$\left(threads-2\right)$}. Another strategy to reduce the load of data transmission is to write map values to the HDFS files, only to send \emph{$<key, filename\#offset>$} pairs.

\item Combine阶段：在Combine阶段，对同一节点上的mappers产生的数据进行聚合，同一个key的value 值进行聚合，这将会大大减少一个节点上的网络传输量，从而提高程序性能。
%Combine Phase: Aggregating the output pairs which generated by map tasks with the same key on the same node will reduce the network traffic
%across nodes, and improve the efficiency of the program.

\item Partition阶段：由于mapper产生的输出道的key 值为输出道道号，Hadoop会自动按文本来排序，而不是按照数字大小排序，所以，在mapper发送到reduce端时，进行一个映射，把道号按照reduce 任务的大小进行划分，reduce id小的对应道号小的输出道。映射公式为：
    \emph{\[f_{hash}=[\frac{key}{\left(\frac{key_{max}}{reduce_{id}}\right)}]\]}。 这个策略会减少最后整个输出道的排序时间，只需要在每个reduce中进行排序，这同时也进行了并行的排序。
%Partition Phase: Mapping the output \emph{$<key,value>$} pairs to the reduce nodes with the key to ensure that all keys in the reduce node are smaller than all the keys in another reduce node which its reduce task attempt id is larger than the previous one. The formula is as follows: \emph{\[f_{hash}=[\frac{key}{\left(\frac{key_{max}}{reduce_{id}}\right)}]\]} This strategy will reduce the final
%    large data sort time. We will only sort the keys in each reduce task in parallel and this also reduce the application execution time.

\item Reduce阶段：对mapper中传来的输出道\emph{$<key,filename\#offset>$}键值对，我们根据键值对读取mapper生成的文件，进行聚合相同的key值，对key 值相同的value 进行叠加。
%Reduce Phase: According to the output \emph{$<key,filename\#offset>$} pairs which sent by map tasks, we read HDFS files according to \emph{filename, offset} and aggregate them with the same key.
%All the reduce tasks run in parallel.

\item Output阶段：在Hadoop中，每个reduce 会对应一个输出文件，Hadoop中默认的是把reduce的输出键值对按文本的形式输出到文件，这里我们对reduce 重写输出文件类：\emph{\small{FileOutputFormat}}，在每个输出类中，我们获取键值对并排序，按照排好的序列将value值写入二进制文件，并把文件名用最小的key值命名。
%Output Phase: Each reduce task produces an output \emph{$<key,value>$} pairs, we sort these keys in each reduce task, then write them to a
%binary file, the file name contains the minimum key in each reduce task.

\item 成像步骤：根据reduce阶段生成输出文件，我们根据文件名中的最小key 值进行排序，然后依次把所以的二进制文件聚合到一个image二进制文件中。最后删除所有的中间文件以节省空间。
%Image Output: According to the reduce output files, we sort these file names with the minimum key, then write them into only one image binary file sequentially.
\end{enumerate}

\par Hadoop上Kirchhoff算法结构图如图8所示。

\begin{figure}
\renewcommand{\captionfont}{\small}
  \renewcommand{\captionlabelfont}{\small}
\centering
\includegraphics[height=8cm,width=8cm,angle=0]{fig72.eps}
\caption{Hadoop上的Kirchhoff算法流程}
\end{figure}


\subsection{Kirchhoff on Spark}
\par ~~~~Spark提供了弹性分布式数据集（RDD）屏蔽了与HDFS的交互。Spark还开发了ApplicationMaster来适用于Yarn，如图3所示。Spark在Yarn上的部署是最有前景的部署方法。因此我们将在Yarn上运行Spark程序。Spark上的Kirchhoff算法设计如下：
%Spark provides RDD which shields a lot interactions with the HDFS. Spark develops the ApplicationMaster which is appropriated to Yarn as shown in Fig.3.
%Thus we run the the Spark programs on the Yarn framework.
%That means the program put Yarn as its resource management system for resource scheduling.
\begin{enumerate}[1)]
\setlength{\itemindent}{2em}
\item 获取集群环境阶段：Spark提供了\emph{newAPIHadoopFile}方法来获取HFDS上的输入文件，该方法读取HDFS 文件产生的$<key,value>$ 键值对生成一个RDD数据集，\emph{key}代表的是输入道道号，\emph{value}代表的是输入道对应的中心点坐标。一个RDD记录就代表一系列输入道，该RDD的分区会默认是HDFS上输入文件的分块数。RDD的一个分区就对应了一个executor （相当于mapper），多个executor分布在不同节点上并行执行。此外，Spark 提供了一个\emph{spark-submit} 脚本来提交用户程序。该脚本可以通过命令行设置executor的个数\emph{N}，每个executor使用的内存量以及每个executor使用的CPU核数，因此我们只需要读取这些参数就可以知道集群的环境。这种设置非常方便，从这些参数中我们就可以知道RDD 的分区可以设置为：\emph{\[f_{pars}=k*min\left(N,\frac{\sum_{i=1}^n M_i}{M_{min}}\right)\]}
    \emph{k$\in{[1,\infty)}$}是一个可变参数，为一个整数，表示分区数为executor数的倍数，这样就保证了每次都并行处理分区，不会遗留剩余的分区被executor 单独处理。\emph{N}代表这个集群上的executor个数，一般\emph{$1\sim2$}CPU 核一个executor，如果内存特别小，则会影响executor数量，因此选择CPU核心数和内存划分数最小的一个作为executor的数量。
%Get Cluster Environment: The program on Spark need to read data from HDFS, we just simply read splits from HDFS using
%    \emph{newAPIHadoopFile} with whatever splits, produced
%\emph{$<key,value>$} pairs as records of RDD. Because RDD can be partitioned according to user's wishes. One partition corresponds to a executor which is similar to mapper. In addition, Spark provides the command---spark-submit to submit the application, users can set the number of task
%executors \emph{N}, the memory of each executor and the CPU cores of each executorl through the command line, so we just need to read the environment variables from the command line. This is very convenience. Then the partitions of the RDD will be set
%\emph{\[f_{pars}=k*min\left(N,\frac{\sum_{i=1}^n M_i}{M_{min}}\right)\]}

\item 输入阶段：Spark提供的\emph{newAPIHadoopFile} 方法可以读取HDFS文件，并且能通过自定义的输入格式来读取文件，我们定义一个输入文件类，把输入道文件读取成\emph{$<key,value>$}键值对，并且保证每道数据都是一个整体，不能切分一道数据，方便计算。\emph{key}代表输入道道号，\emph{value}代表输入道道号对应的中心点坐标。通过读取HDFS，Spark返回一个RDD数据集，RDD 的每条记录包含一系列输入道，因为RDD支持手动分区，根据上面的公式，我们把RDD 分区以在executor 上并行执行。另外，RDD还支持\emph{persist} 操作，可以把中间数据缓存在内存中，\emph{persist}提供了多级缓存机制，可以把数据完全缓存在内存中，也可以缓存在硬盘上，或者两种都使用。不仅如此，\emph{persist} 操作还支持缓存双份在内存或硬盘中，加速了并行读写速度。\emph{persist}在第一次计算后就缓存了数据，等下次计算时直接读取缓存，加快了执行速度。因此，我们把从HDFS中读取的输入道数据缓存在内存和硬盘中（防止过大，内存不足），当再次读取数据时加快速度，并且能在后续RDD失败时，能快速重建该RDD。
%Input Phase: Because we put the Spark on the Yarn and HDFS framework, we need to read files from HDFS. Spark provides several functions
%to read the input files from HDFS, and return them into RDD model. Each RDD contains many records of \emph{$<key,value>$} pairs, \emph{key} represents the offset of the input trace and \emph{value} represents the coordinate points of the corresponding key. One record combines many traces. \\
% Moreover, RDD can persist data in memory after the first calculation, so we persist those RDD records in memory and hard disk(if it's too large), this prevents repeatedly read from HDFS. Then we partition the RDD with the above formula. These partitions will be calculated in
% parallel.
 %RDD supports several types of persist operations, such as memory, disk, memory and disk together, etc. We read from the HDFS files and turn them into RDD, then RDD's number of partition will be the number of HDFS blocks. This is the bottleneck of the program. However, RDD also supports partition operation, we can repartition the RDD data set in no matter
%how many partitions that you like. This feature gives the program a lot of flexibility. User can try the number of partitions to find out which
%number will bring the best performance. Each partition of the RDD will be compute in parallel, there we realize the parallel operations.

\item FlatMap阶段：在Spark中，RDD的每个分区都会进行\emph{FlatMap}操作，一次一个executor计算一个分区，当分区计算完成后继续计算其他分区，多个executor并行执行。在\emph{FlatMap}阶段，获取\emph{$<key,value>$}键值对中的输入道道号和中心点坐标，对每个输入道读取HDFS上的速度文件，炮点坐标文件和检波器坐标文件来计算出输出道的数据。每个输入道对应一个孔径，每个孔径中有一系列输出道，相邻的输入道产生的输出道有很多交集，因此我们在\emph{FlatMap}中使用一个\emph{HashMap} 来保存输出道，相同的输出道叠加到一起。这有利于减少节点之间和节点内的通信量。因为每个输入道会产生大量的输出道，一个\emph{FlatMap}包含了大量的输入道，因此我们用个多线程来并行加速一个RDD分区，如果机器系统开启了
    \emph{Hyper-Threading}，则设置线程数为\emph{$2*\left(threads-2\right)$}，如果\emph{Hyper-Threading} 关闭，则设置线程数为
    \emph{$\left(threads-2\right)$}。在Hadoop中，当mapper任务运行到一个合适的百分比时，系统会启动reduce任务来收集mapper任务产生的中间结果，reduce 任务会一直占用资源，这会影响后期mapper的数量。使得程序性能下降。而在Spark 中，当mapper任务结束后会返回一个RDD，然后这个RDD被应用到reduce操作，因此，系统不会在mapper没执行完时进行reduce操作，这使得mapper 可以完全并行执行，提高了程序的性能。另外，在Spark中，一旦RDD分区后，分区信息会一直保留着，也就是说，在执行reduce操作时，RDD 的分区数依然是mapper规定的分区数。如果mapper产生的分区数不适合reduce任务，则可以使用
    \emph{repartition}函数对RDD进行重新分区来适应reduce任务。在这阶段，对输出道进行聚合生成\emph{$<key,value>$}键值对，\emph{key}代表输出道道号，
    \emph{value}代表输出道对应的成像数据。返回这些键值对生成的RDD。
%FlatMap Phase: In Spark, RDD partitions will be sent to executors. One time a executor calculates a partition, when it's done, it continues with the remaining partitions. All of the executors calculate in parallel. In this map function, we also use the multi-threads
%    to compute the input traces. Because Spark is a threading model. Whatever the cluster system opens \emph{Hyper-Threading}, we just set the threads  number as \emph{$\left(threads-2\right)$} to ensure the good performance. In Hadoop, when map function running a proper percent(such as 5\%), reduce tasks will be launch to collect output pairs from mappers. But in Spark, reduce tasks starts until mappers finished. Mappers return the output pairs directly to the reduce tasks. This feature make the program on Spark more efficient. It's worth mention that the partitions of the RDD do not change until you use the \emph{repartition} function.
     %RDD partition sends the <key,value> pairs to the executor, executor handles these input trace
%keys and produces output traces of each input trace. In each executor we sequentially process the input trace keys which tasks a lot of time,
%so in each executor we use multi-threads to process the input traces that shortens the total executor time. However, the number of threads
%depends on the cluster environment, we can not use too many threads. Every executor produce an output RDD, then Spark aggregate them into a
%big RDD. The RDD will be returned to further use. It's worth mentioning that the number of the RDD parititions does not change until you use the repartition operation.

\item Partition阶段：首先，我们获取输出道的总大小\emph{onx}，然后根据输出道的\emph{key}值进行划分，保证小的\emph{key}值对应同一个reduce 任务，这样就避免了最后成像数据的排序时间，只需要在每个reduce任务中并行排序就行。我们根据reduce任务的个数\emph{$R_n$}来确定每个\emph{key}的映射情况，公式如下：\emph{\[f_{partition}=[\frac{key}{\left(\frac{onx}{R_n}\right)}]\]}
%Partition Phase: Firstly, we get the total number of output trace \emph{onx}. Then we divide these keys depending on the number of reduce partitions \emph{$R_n$}. Small keys correspond to the small reduce partition id. This helps the later sort operation. Each record in RDD apply the mapping
%    operation to choose which reduce partition to go. The formula as follows:\emph{\[f_{partition}=[\frac{key}{\left(\frac{onx}{R_n}\right)}]\]}

\item ReduceByKey阶段：该阶段接收来自映射的FlatMap的\emph{$<key,value>$}键值对，对每个相同的\emph{key}值的\emph{value}进行叠加，生成图像数据。
\emph{reduceByKey}方法的另一个优化就是：它会先把同一个节点的相同输出道道号\emph{key}进行聚合，然后再传递到reduce任务中，这减少了节点间的通信量。\emph{reduceByKey}方法将返回一个RDD，保存了输出道道号和成像数据组成的键值对。每个reduce任务会并行的运行在不同的executor上，一个reduce任务完成后就会执行下一个reduce任务，只要保证reduce分区是reduce任务的倍数，这样才能保证所有的reduce任务并行执行，不会浪费资源。
%ReduceByKey Phase: Spark ensures that the same key pairs will be sent to the same reduce task. According to this feature, the RDD datasets that come from map operation are sent to the same reduce tasks with the same key. So we aggregate the values by the same key in each reduce
%executor. Each reduce executor return a RDD partition which will be aggregated into a total RDD to the user, however, its partitions still
%exist. The important point of the \emph{ReduceByKey} function is that the operation firstly merges keys locally, then send pairs to the correspond
%reduce executor. This feature greatly reduces the load of network traffic.

\item SortByKey阶段：在Reduce阶段生成的RDD进行排序操作，因为生成的成像文件是按照输出道道号的大小进行排序的，运用Spark的\emph{sortByKey}函数可以对每个RDD分区中的输出道进行聚合。所有的分区都是并行排序，加快了排序时间。最后返回排序好的RDD。
%SortByKey Phase: This function sorts the keys in each RDD partition which reduce executors returned. The sort operation is also in parallel
%with the different RDD partitions.

\item 成像阶段：根据排序好的RDD进行写文件到HDFS，Spark提供了\emph{saveAsNewAPIHadoopFile} 函数对RDD 分区分别写入文件，产生的文件名由该分区中最小的输出道道号的值来命名，最后根据文件名来合并所有的成像文件，生成一个二进制成像文件，并删除中间文件。
%Image Output: According to the sorted keys, we write the corresponding values to a binary file to HDFS for permanent perservation.

\end{enumerate}

\par 这个程序的流程图如图9所示。
%The program flow chart shown in Fig.8.

\begin{figure}[H]
\renewcommand{\captionfont}{\small}
  \renewcommand{\captionlabelfont}{\small}
\centering
\includegraphics[height=8cm,width=8cm,angle=0]{fig81.eps}
\caption{Spark框架上的Kirchhoff实现}
\end{figure}


\section{实验评价}

\subsection{实验环境}
\par ~~~~集群有4个节点，分别是\emph{Master,Slave1,Slave3,Slave4}。集群每个节点的资源如下表所示。
The cluster has four nodes which include Master, Slave1, Slave3, Slave4. The resources of the Cluster shown in TABLE.1.


\begin{center}
\begin{table}[htbp]
\renewcommand{\captionfont}{\small}
  \renewcommand{\captionlabelfont}{\small}
\centering
\begin{tabular}{ | c | c | p{1.6cm}<{\centering} | p{1.6cm}<{\centering} | c |}
\hline
Name &CPUs &Cores Per CPU &Thread Per Core &Memory(G) \\
\hline
Master &2 &8 &4 &32 \\
\hline
Slave1 &2 &6 &4 &32 \\
\hline
Slave3 &2 &6 &4 &32 \\
\hline
Slave4 &2 &6 &4 &32 \\
\hline
\end{tabular}
\caption{Cluster Configuration}
\end{table}
\end{center}

在集群中每个节点只配置20G内存给Yarn做资源，保证节点上其他程序和系统运行流畅。

\subsection{实验配置}
\par ~~~~“Master”节点作为Hadoop的Master节点，用于调度集群中的任务，不进行计算（但当我们需要测试四台机器性能时，也会把该节点扩展成计算节点），该节点主要用于运行HDFS的管理进程NameNode以及Yarn的管理进程ResourceManager；其他“Slave1”、“Slave3”、“Slave4” 三个节点作为作为Hadoop的Slave节点，这些节点主要用于运行HDFS的存储进程DataNode和Yarn的节点任务管理进程NodeManager。
%The node "Master" acts as the Hadoop Master node and the remaining nodes act as Hadoop Slave nodes. "Master" is not only the
%master of the HDFS framework(act as NameNode), but also the master of the Yarn framework(act as ResourceManager). That means that we act the
%"Slave1", "Slave3", "Slave4" as the DataNode and the NodeManager(\emph{When we need 4 compute nodes, we also include "Master" as a Slave node}).
%"Master" charges the resources manager, whereas "Slave1,3,4" run the computing
%tasks on themselves.

\subsection{数据准备}
\par ~~~~PKTM主要包括三个方面：数据预处理，数据迁移，输出成像。PKTM使用两种输入数据文件格式，包括“meta”文件和“data”文件。在本程序中输入文件主要包括：输入道meta文件（shot.meta），输入道震源坐标文件（fsxy.meta），输入道检波器坐标文件（fgxy.meta），输入道中心点坐标文件（fcxy.meta），速度信息文件（rmsv.meta）。每个meta信息文件都对应有一个data文件，用于保存数据文件：shot.data，fsxy.data，fgxy.data，fcxy.data,rmsv.data。
%PKTM includes the following three parts: data preprocessing, migration, output. PKTM use two input data file formats including "meta" files and "data" files\cite{GPU}. The input files in this program contains input trace meta
%file(shot.meta), input trace seismic source meta file(fsxy.meta), input trace detector location meta file(fgxy.meta), input trace center
%points meta file(fcxy.meta), velocity meta file(rmsv.meta). Each of the meta file correspond to the data file(*.data), such as shot.data,
%fsxy.data, fgxy.data, fcxy.data, rmsv.data.

\subsection{实验结果}
\par ~~~~我们主要从以下几个方面来测试我们的程序：Hadoop上的单独测试、Spark的单独测试、Hadoop和Spark对比测试。为了节省时间，我们使用了小数据集进行测试，对于同一个测试使用同一个数据集，但是对于不同的测试可能使用的不是同样大小的数据集。
%We experiment our program as the following aspects:
\begin{enumerate}[1.]
\setlength{\itemindent}{2em}
\item 在Hadoop上的测试，
%Experiments on Hadoop (\textbf{The same test uses the same data, a different test may use different data. That means we use a small data to test some situation in order to save time}):
\begin{enumerate}[a.]
\setlength{\itemindent}{2em}
\item 我们首先测试mapper任务的container内存量变化给程序性能带来影响。测试结果如图10所示。我们可以看出当内存扩展到一个阈值后，mapper的数量就由于内存的增大而变少，因此执行时间就会变长。
%We firstly test how the memory of a mapper's container to affect the performance of the program. The test results shown in Fig.9. We can see that when the memory exceeds to a threshold, the number of mappers reduces, then the execution time become longer.
\begin{figure}
\renewcommand{\captionfont}{\small}
  \renewcommand{\captionlabelfont}{\small}
\centering
\includegraphics[height=5cm,width=7cm,angle=0]{fig91.eps}
\caption{Mapper内存变化}
\end{figure}

\item 其次，我们测试mapper的数量如何影响程序的性能。程序测试结果如图11所示。在这幅图中，当我们的mapper数量适应了集群的资源后，就获得了最佳的执行时间，而其他的数量就会获得相对较差的性能。
%Secondly, we test the number of mappers how to affect the performance. The test results shown in Fig.10. In the figure, we know that
%when the mapper's numbers adapter to the cluster's resources, it gets the best execution time. Others get a worse performance.
\begin{figure}
\renewcommand{\captionfont}{\small}
\renewcommand{\captionlabelfont}{\small}
\centering
\includegraphics[height=5cm,width=7cm,angle=0]{fig92.eps}
\caption{Mappers数量变化}
\end{figure}

\item 最后，在Hadoop中，当mapper完成一定的百分比时，集群就会启动reduce任务来接收mapper的输出文件，但是当reduce任务过早启动的话，会一直占用集群的内存和CPU资源，使得后来执行的mapper任务数量大大的减少，如果reduce任务数量较多，最后mapper会剩下很多单独执行的任务，失去了并行执行的优势。因此，我们测试reduce任务启动的时间对程序性能的影响。测试结果如图12所示。在图中，我们看到了当启动任务的时间达到一个合适的值时，程序会达到一个好的性能。
%Finally, in Hadoop, when mappers have not finished, the cluster will set up reduce tasks to receive map output files. But the started reduce
%tasks hold the resources of memory and CPUs which effects the mappers' numbers and execution. Then, we test how reduce task numbers affects the performance. The results
%shown in Fig.11. In the figure, we know that when the numbers of reduce tasks match the number and output pairs of mappers, it gets the best performance.
\begin{figure}
\renewcommand{\captionfont}{\small}
\renewcommand{\captionlabelfont}{\small}
\centering
\includegraphics[height=5cm,width=7cm,angle=0]{fig11.eps}
\caption{Reduces数量变化}
\end{figure}

\end{enumerate}
\item 在Spark上的实验：在Spark中，我们只测试两个方面，每个executor的内存量和RDD的分区数。
%Experiments on Spark: in Spark, we only test two aspects, memory of each execution(container) and number of RDD partitions.
\begin{enumerate}[a)]
\setlength{\itemindent}{2em}
\item 我们首先改变container内存的配置来测试程序的性能，测试结果如图13所示。像Hadoop一样，如果container的内存量超过一个阈值后，程序的执行时间会变得更长。
%We change the configuration of the container's memory to evaluate the performance of the program. The results shown in Fig.12. Like Hadoop, if the memory of a container exceeds a proper value, the execution time grows longer.
\begin{figure}
\renewcommand{\captionfont}{\small}
\renewcommand{\captionlabelfont}{\small}
\centering
\includegraphics[height=5cm,width=7cm,angle=0]{fig12.eps}
\caption{Container Memory}
\end{figure}

\item 在本部分，我们测试输入道数据的RDD分区，测试结果如图14所示。图中显示了当RDD分区不足集群所能容纳的最大资源数时，程序的性能是上升的，但当RDD的分区继续增加时，程序的性能会下降，但当分区超过集群资源容纳的executor数量时，性能的变化不大，因为所有分区都是并行执行。测试结果如图14 所示。
%In this part, we test the RDD partitions about the input traces, find the best partitions. The test shown in Fig.13. The figure indicates that when RDD partitions is not enough to the cluster, it takes a lot of time, but when partitions grows, it tends to a
%    stable execution time. The reason of this phenomenon is that when RDD partition is more than the cluster resources can have, it makes the
%    cluster run busy with the same execution time.
\begin{figure}[H]
\renewcommand{\captionfont}{\small}
\renewcommand{\captionlabelfont}{\small}
\centering
\includegraphics[height=5cm,width=7cm,angle=0]{fig13.eps}
\caption{RDD partitions}
\end{figure}

\end{enumerate}

\item Hadoop和Spark上的程序测试对比：在本次测试中，我们用了相同的数据集，以及大致相同的配置，如container的内存量，mapper中的线程数等。测试结如图15所示。
%Experiments compared to Hadoop and Spark:
\begin{figure}
\renewcommand{\captionfont}{\small}
  \renewcommand{\captionlabelfont}{\small}
\centering
%\includegraphics[height=7cm,width=8cm,angle=0]{fig9.eps}
\includegraphics[height=5cm,width=7cm,angle=0]{fig10.eps}
\caption{Hadoop和Spark性能对比}
\end{figure}
\end{enumerate}

%\begin{enumerate}[1)]
%\setlength{\itemindent}{2em}
%\item Four slaves(Master, Slave1, Slave3, Slave4), one Master node and four Slave nodes. We
%set 1000 input trace per mapper and 4 threads per mapper. In this case, Hadoop program takes
%38 minutes and 4 seconds, while Spark program takes 15 minutes and 36 seconds.
%
%\item Three slaves(Slave1, Slave3, Slave4), one Master node and three Slave nodes. We set 1000 input trace per mapper and 4 threads per mapper. In this case, Hadoop program takes 48 minutes and 18 seconds, Spark program takes 28 minutes and 26 seconds.
%
%\item Two slaves(Slave1, Slave3), one Master node and two Slave nodes. We also set 1000 input trace per mapper and 4 threads per mapper.
%In this case, Hadoop program takes 1hour, 15minutes and 15seconds, Spark program takes 38 minutes and 48 seconds.
%
%\item One slave(Slave1), one Master node and the other one is the Slave node. In this case, Hadoop program takes 2 hours, 42 minutes and 24 seconds, Spark program takes 1 hour, 16 minutes and 32 seconds.
%\end{enumerate}

%\par Test results of the program are as follows:




% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results.}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
% However, the Computer Society has been known to put floats at the bottom.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals use top floats exclusively.
% However, Computer Society journals sometimes do use bottom floats - bear
% this in mind when choosing appropriate optional arguments for the
% figure/table environments.
% Note that, LaTeX2e, unlike IEEE journals, places footnotes above bottom
% floats. This can be corrected via the \fnbelowfloat command of the
% stfloats package.




\section{总结}
\par ~~~~在本文中，Kirchhoff算法的设计目的主要是避免HDFS读写和输入道计算分开，因为CPU的计算执行时间要比HDFS读写速度快很多。当CPU计算时如果需要数据来计算，则会等待HDFS读写完毕后继续执行计算，这会导致CPU的一个读写等待时间，浪费了CPU的资源。因此我们会尽量让数据事先读到内存中，这样当CPU需要数据进行计算时，会从内存中读取数据，速度远远小于直接读取HDFS文件。尽管我们做了这样的改善，但当内存量不够的情况下，程序依旧需要从HDFS读取文件，然后缓存到内存中，等CPU计算时获取。在Spark中，提供了RDD内存计算，我们可以直接把数据读取到RDD中，然后缓存到内存或硬盘上，当CPU计算需要数据时，直接从缓存的内存或硬盘中读取，加快了读写的速度，这个程序依然适用于大数据，只需要修改些配置就行了。
\par 本程序的性能和集群的性能有关，如果集群有网络高通量和高速的硬盘读写能力，程序会有更好的性能。这意味着，网络通量和硬盘的读写能力是程序的一个性能瓶颈。另一个程序的瓶颈就是程序中依然存在着CPU计算和HDFS读写的交叉执行。还有一个瓶颈就是内存的容量，当内存的容量特别大时，程序每次可以缓存更多的数据到内存中，这样就减少了读取HDFS的次数，提高了程序的性能。
\par 本段中我们介绍一些Kirchhoff改进的想法，在Hadoop中，我们通过Infiniband在HDFS上应用了RDMA（Remote Direct Memory Access）技术，这将在很大程度上提高程序的性能。这是解决网络传输慢的瓶颈的一个方法。同样的，在Spark中，我们也可以应用Infiniband高速网卡来加速网络传输。Spark还可以使用Tachyon来加速访问HDFS文件，Tachyon完全兼容HDFS，搭建在HDFS上层。Tachyon是一个高容错的分布式文件系统，允许文件以内存的速度在集群框架中进行可靠的共享，就像Spark和MapReduce那样。通过利用信息继承，内存侵入，Tachyon获得了高性能。Tachyon工作集文件缓存在内存中，并且让不同的Jobs/Queries以及框架都能以内存的速度来访问缓存文件。
%In this paper, the design of the algorithm tries hard to avoid the cross execution of reading HDFS files and computing traces.
%Because the computing time is faster than the reading time. Time of these two operations is not an order of magnitude. CPU always
%wait to read HDFS files before computing. This wastes a lot of time. We should firstly read the data into memory, then compute them when
%they needed. In spite of Spark provides RDD to accelerate memory computing, this program is not suitable for iterative way. So it just a
%little faster than the program runs in Hadoop. This program is also suitable for the huge seismic data that just need to alter some
%configurations.
%\par The efficiency of this program is closely related to the machine's performance. If the machine has a high throughput of the network
%and the high-speed hard disk I/O, the program will run faster. That means the network traffic and hard disk I/O are the bottleneck of the
%program. The other bottleneck of the program is that there still exists some waiting time wasted if the data size exceeds memory size, CPU
%need to wait to read HDFS files before computing.
%\par In Hadoop, we can apply RDMA(Remote Direct Memory Access) in HDFS through Infiniband\cite{RDMA}. This will greatly promote the acceleration of
%HDFS read and write time. This is one way to solve the bottleneck. While, in Spark, we can also apply Infiniband Network Interface Card to accelerate
%the network transfer. We can also use Tachyon as the distributed file system\cite{TachyonSpark,TachyonSpark2}. Tachyon is a memory-centric distributed storage system enabling reliable data sharing at memory-speed across cluster frameworks, such as Spark and MapReduce. It achieves high performance by leveraging lineage information and using memory aggressively. Tachyon caches working set files in memory, thereby avoiding going to disk to load datasets that are frequently read. This enables different jobs/queries and frameworks to access cached files at memory speed\cite{Tachyon}. It is totally compatible with HDFS.





% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.


% use section* for acknowledgement
%\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  %\section*{Acknowledgments}
%\else
  % regular IEEE prefers the singular form
  %\section*{Acknowledgment}
%\fi

%\par ~~~~My deepest gratitude goes first and foremost to my supervisor, Tang Jie, for his constant encouragement and guidance. He has walked
%me through all the stages of the writing of this thesis.
%\par Second, I would like to express my heartfelt gratitude to the classmates in MCG(Multimedia Computing Group).



% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)


%\bibliographystyle{IEEEtran}

%\bibliography{./IEEEexample}

\begin{thebibliography}{24}
%\bibitem{BigData}
%ChengXueQi, JinXiaoLong, WangYuanZhuo, etc. Summary of the Big Data systems and analysis techniques[J] in chinese. Journal of Software, 2014, 25(9).
%
%\bibitem{HadoopMemory}
%Gu L, Li H. Memory or Time: Performance Evaluation for Iterative Operation on Hadoop and Spark[C]//High Performance Computing and Communications\& 2013 IEEE International Conference on Embedded and Ubiquitous Computing (HPCC\_EUC), 2013 IEEE 10th International Conference on. IEEE, 2013: 721-727.
%
%\bibitem{ApacheHadoop}
%Apache Hadoop. [Online]. Available: http://hadoop.apache.org
%
%\bibitem{MapReduce}
%Dean J, Ghemawat S. MapReduce: simplified data processing on large clusters[J]. Communications of the ACM, 2008, 51(1): 107-113.
%
%\bibitem{MapReduce2}
%Zaharia M, Konwinski A, Joseph A D, et al. Improving MapReduce Performance in Heterogeneous Environments[C]//OSDI. 2008, 8(4): 7.
%
%\bibitem{Yarn1}
%Vavilapalli V K, Murthy A C, Douglas C, et al. Apache hadoop yarn: Yet another resource negotiator[C]//Proceedings of the 4th annual Symposium on Cloud Computing. ACM, 2013: 5.
%
%\bibitem{HDFS1}
%Ghemawat S, Gobioff H, Leung S T. The Google file system[C]//ACM SIGOPS operating systems review. ACM, 2003, 37(5): 29-43.
%
%\bibitem{HDFS2}
%Borthakur D. HDFS architecture guide[J]. Hadoop Apache Project, 2008: 53.
%
%\bibitem{Spark1}
%Apache Spark. [Online]. Available: http://spark.apache.org
%
%\bibitem{Spark2}
%M. Zaharia, M. Chowdhury, S. S. Michael J. Franklin, and I. Stoica, “Spark: Cluster computing with working sets,” In HotCloud, June 2010.
%
%\bibitem{Spark3}
%Zaharia M, Chowdhury M, Das T, et al. Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing[C]//Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation. USENIX Association, 2012: 2-2.
%
%\bibitem{PKTM}
%Shi X, Wang X, Zhao C, et al. Practical Pre-stack Kirchhoff Time Migration of Seismic Processing on General Purpose GPU[C]//CSIE (2). 2009: 461-465.
%
%\bibitem{GPU}
%WangGang, TangJie, WuGangShan. GPU-based Cluster Framework[J] in chinese. Computer Science and Development, 2014, 24(1): 9-13.

\bibitem{Parallel}
Rizvandi N B, Boloori A J, Kamyabpour N, et al. MapReduce implementation of prestack Kirchhoff time migration (PKTM) on seismic data[C]//Parallel and Distributed Computing, Applications and Technologies (PDCAT), 2011 12th International Conference on. IEEE, 2011: 86-91.

\bibitem{KirchhoffMapReduce}
Emami M, Setayesh A, Jaberi N. Distributed computing of Seismic Imaging Algorithms[J]. arXiv preprint arXiv:1204.1225, 2012.

\bibitem{GPGPU}
De Verdiere G C. Introduction to GPGPU, a hardware and software background[J]. Comptes Rendus Mécanique, 2011, 339(2): 78-89.

\bibitem{KirchhoffAccelerating}
Panetta J, Teixeira T, de Souza Filho P R P, et al. Accelerating Kirchhoff migration by CPU and GPU cooperation[C]//Computer Architecture and High Performance Computing, 2009. SBAC-PAD'09. 21st International Symposium on. IEEE, 2009: 26-32.

\bibitem{GPUMapReduce}
Gao H, Tang J, Wu G. A MapReduce Computing Framework Based on GPU Cluster[C]//High Performance Computing and Communications \& 2013 IEEE International Conference on Embedded and Ubiquitous Computing (HPCC\_EUC), 2013 IEEE 10th International Conference on. IEEE, 2013: 1902-1907.

%\bibitem{Seismic}
%Rizvandi N B, Zomaya A Y, Zomaya A Y. HIGH PERFORMANCE COMPUTING OF HIGH PERFORMANCE COMPUTING OF SEISMIC DATA ON MAPREDUCE[J].

%\bibitem{RDMA}
%Lu X, Islam N S, Wasi-ur-Rahman M, et al. High-performance design of Hadoop RPC with RDMA over InfiniBand[C]//Parallel Processing (ICPP), 2013 42nd International Conference on. IEEE, 2013: 641-650.
%
%\bibitem{TachyonSpark}
%Li H, Ghodsi A, Zaharia M, et al. Tachyon: Memory throughput i/o for cluster computing frameworks[J]. memory, 2013, 18: 1.
%
%\bibitem{TachyonSpark2}
%Li H, Ghodsi A, Zaharia M, et al. Tachyon: Reliable, memory speed storage for cluster computing frameworks[C]//Proceedings of the ACM Symposium on Cloud Computing. ACM, 2014: 1-15.
%
%\bibitem{Tachyon}
%Tachyon. [Online]. Available: http://tachyon-project.org/

%\bibitem{Kirchhoff}
%D.Bevc, "Imaging Complex Structure with SmiRecursive Kirchhoff Migration," Geophysics, vol. 62, pp. 577-588, 1997.

\end{thebibliography}


% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the IEEEhowto:kopkaIEEEhowto:kopkacomplicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}
%Biography text here.
%\end{IEEEbiography}

% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}
\end{CJK*}

% that's all folks
\end{document}



